
\noindent We left off with an object meant to be the classical version of a fermion: a Grassman number. It is the object of a vector space $ \mathcal{G}_n (V)$, generated by basis $\{ 1, \theta_1, \dots, \theta_n \in V \}$. Imposing the anticommutation relation 

\begin{equation}
\{\theta_j, \theta_k \} = 0, \, \forall j,k = 1, \dots, n
\end{equation}

\noindent This is a $2^n$-dimensional vector space with monomial basis

\begin{equation}
\{ 1, \theta_1, \theta_2, \dots, \theta_n, \theta_1 \theta_2, \dots , \theta_{n-1} \theta_n, \dots , \theta_1 \theta_2 \dots \theta_n \}.
\end{equation}

\noindent An arbitrary element $f \in \mathcal{G}_n (V)$

\begin{equation}
f = f_0 + \sum_{j_1 < \dots < j_p} f_p (j_1, \dots , j_p) \, \theta_{j_1} \dots \theta_{j_p}
\end{equation}

\noindent We now define functions, linear and nonlinear, representations, complex numbers, calculus, derivatives, and integrals of Grassman numbers.

\subsection*{Functions of Grassman Numbers}

\noindent Think of $\theta_j$ as anticommuting numbers/variables. \\

\noindent Note: A "wrong" definition is to let $f$ be an infinitely differentiable function $f \in C^\infty (\mathbb{R})$, adjoin the symbol $f(\theta_j)$, and impose the anticommutation relation $\{ f(\theta_j), \theta_k \} = 0, \, \forall \, j, k$. Then each $f$ will produce a new anticommuting object, which leads to an uncountably infinite number of objects, and is not what we expect from a function. \\

\noindent Linear functions should be linear maps from the vector space to itself, $F \in \mathcal{M}_{2^n} (\mathbb{C})$, the space of $2^n \times 2^n$ matrices

\begin{equation}
F : \,\, \mathcal{G}_n (V) \rightarrow \mathcal{G}_n (V)
\end{equation}

\noindent Nonlinear functions will be defined by analogy to the functional calculus of matrices; consider a matrix $M \in \mathcal{M}_m (\mathbb{C})$. As long as $M$ is diagonalizable, define the nonlinear function $f(M) = S^{-1} f(D) S$, where $M$ is diagonalized by S, such that $M = S^{-1} D S$. Since a diagonal matrix occupies a commutative algebra, we know how to define function for $D$ and then rotate from $D$ to $M$ via $S$. \\

\noindent In summary so far, one strategy to define functions of Grassman numbers is to represent $\mathcal{G}_n (V)$ as matrices and use functional calculus. Working with representations, we now need matrices to represent the Grassman numbers.

\subsection*{Representations of $\mathcal{G}_n (V)$}

\noindent Consider a map from our vector space to a concrete space of matrices

\begin{equation}
\pi : \,\, \mathcal{G}_n (V) \rightarrow M_d (\mathbb{C}
\end{equation}

\noindent Which must obey the anticommutation relation

\begin{equation}
\{ \pi (\theta_j), \pi (\theta_k) \} = 0, \,\, \forall j,k .
\end{equation}

\noindent Construct the "Jordan-Wigner representation", which will look very familiar. Begin with a Hilbert space $\mathcal{H} = \mathbb{C}^{2^n}$, with dimension $2^n$, and Pauli operators 

\begin{align}
\sigma^+ &= \left( \begin{matrix} 0 & 0 \\ 1 & 0 \end{matrix}  \right) \\
\sigma^z &= \left( \begin{matrix} 1 & 0 \\ 0 & -1 \end{matrix} \right).
\end{align}

\noindent Recall that these Pauli operators obey the relations

\begin{align}
\{ \sigma^+ , \sigma^z \} &= 0 \\
(\sigma^+ )^2 &= 0.
\end{align}

\noindent Construct the representation of $\mathcal{G}_n (V)$

\begin{align}
\pi (\theta_1 ) &= \sigma_1^+ \otimes \mathbb{I}_2 \otimes \dots \otimes \mathbb{I}_{n-1} \otimes \mathbb{I}_n \\
\pi (\theta_2 ) &= \sigma_1^z \otimes \sigma_2^+ \otimes \dots \otimes \mathbb{I}_{n-1} \otimes \mathbb{I}_n \\ 
&\dots \\
\pi (\theta_n) &= \sigma_1^z \otimes \sigma_2^z \otimes \dots \otimes \sigma_{n-1}^z \otimes \sigma_n^+
\end{align}

\subsubsection*{Single variable representation ($n=1$)}

\noindent Let $F \in C^\infty (\mathbb{R}, \mathbb{R})$ and $\mathcal{G}_1 (V) \simeq \{ a+b \theta : \, a,b \in \mathbb{C} \}$. This should be consistent with the functional calculus of the representation $\pi(\cdot)$, such that $F(\pi(\cdot)) = S^{-1} F(D) S$. 

\noindent To evaluate the function $F$, we write out the Taylor series, evaluate at $x=\theta$, and impose the defined relations (e.g., $\theta_j^2 = 0, \, \forall j$)

\begin{align}
F(x) &= \sum_{j=0}^\infty \frac{F^{(j)} (x=0) x^j}{j!} \\
F(\theta) &= \sum_{j=0}^\infty \frac{F^{(j)} (\theta=0) \theta^j}{j!} \\
F(\theta) &= F^{(0)} (\theta=0) + F^{(1)} (\theta=0) \cdot \theta
\end{align}

\textbf{Example 1:}

\begin{align}
F(x) &= \sin(x) \\
F(\theta) &= (x - \frac{x^3}{3!} + \dots ) \big{|}_{x=\theta} \\
F(\theta) &= \theta
\end{align}

\textbf{Example 2: }

\begin{equation}
F(x) = x + x^3 \rightarrow F(\theta) = \theta
\end{equation}

\subsubsection*{Multiple variable representation}

\noindent Let $F \in C^\infty (\mathbb{R}^n, \mathbb{R})$, with the Taylor expansion

\begin{equation}
F(\theta_1, \dots , \theta_n) = F^{(0)} (\theta_1=0, \dots , \theta_n=0) + \sum_{j=1}^n \theta_j \frac{\partial F(0, \dots , 0)}{\partial \theta_j} + \mathcal{O}(\theta^2)
\end{equation}

\textbf{Example 1: $n=2$}

\begin{align}
F(x,y) &= e^{-\lambda x y} \\
F(\theta_1, \theta_2) &= (1 - \lambda x y + \lambda^2 x^2 y^2 + \dots ) \big{|}_{x=\theta_1, y=\theta_2} \\
F(\theta_1, \theta_2) &= 1 - \lambda \theta_1 \theta_2
\end{align}

\textbf{Example 2: $n=2$}

\begin{align}
F(x,y) &= e^{-\lambda_1 x - \lambda_2 y} \\
F(\theta_1, \theta_2) &= (1 - \lambda_1 \theta_1 ) ( 1 - \lambda_2 \theta_2) \\
F(\theta_1, \theta_2) &= 1 - \lambda_1 \theta_1 - \lambda_2 \theta_2 + \lambda_1 \lambda_2 \theta_1 \theta_2
\end{align}

\noindent Note that even though we lose higher order terms such as $\theta_j^2$, nonlinear features are preserved in the multivariable case.

\subsubsection*{Complex Grassman Numbers}

\noindent Let $\theta_1$, $\theta_2 \in \mathcal{G}_2 (V)$, a 4-dimensional vector space with basis $\{1, \theta_1, \theta_2, \theta_1 \theta_2 \}$, and define the quantities $\theta$ and $\theta^* \in \mathcal{G}_2 (V)$ as

\begin{align}
\theta &= \frac{\theta_1 + i \theta_2}{\sqrt{2}} \\
\theta^* &= \frac{\theta_1 - i \theta_2}{\sqrt{2}}.
\end{align}

\noindent To extend to the multivariable case, define

\begin{equation}
\theta_j^* = \frac{\theta_{j_1} - i \theta_{j_2}}{\sqrt{2}}
\end{equation}

\noindent Where $\theta_{j_1}, \, \theta_{j_2} \in \mathcal{G}_{2n} (V)$.

\subsubsection*{Grassman Derivatives}

\noindent Define the derivative with respect to Grassman variables as the map

\begin{equation}
\partial_{\theta_j} : \,\, \mathcal{G}_n (V) \rightarrow \mathcal{G}_n (V)
\end{equation}

\noindent Which obeys the relations

\begin{enumerate}
\item $\partial_{\theta_j} (\theta_k) = \delta_{jk}$
\item $\partial_{\theta_j} (\theta_{k_1} \dots \theta_{k_p}) = \delta_{j k_1} \theta_{k_2} \dots \theta_{k_p} - \delta_{j k_2} \theta_{k_1} \theta_{k_3} \dots \theta_{k_p} + \dots + (-1)^{p-1} \delta_{j k_p} \theta_{k_1} \dots \theta_{k_{p-1}}$.
\end{enumerate}

\noindent The second relation follows from the anticommutation relation, and can be thought of as bringing the corresponding $\theta_j$ to the front via anticommutations and then differentiating. \\
\noindent For example,

\begin{equation}
\partial_{\theta_2} (\theta_1 \theta_2 ) = - \partial_{\theta_2} (\theta_2 \theta_1) = - \theta_1.
\end{equation}

\noindent (\textbf{Exercise}) These relations can be extended by linearity for any Grassman number. This definition of $\partial_{\theta_j}$ obeys the product rule and the chain rule.

\subsubsection*{Grassman Integrals}

\noindent Following the analogy of the common definite integral, the Grassman integral should be a linear map which obeys shift invariance, such that $\theta \rightarrow \theta + \eta$,

\begin{equation}
\int d\theta : \,\, \mathcal{G}_n (V) \rightarrow \mathbb{C}.
\end{equation}

\noindent The only consistent definition to satisfy these two constraints for a single Grassman variable is 

\begin{equation}
\int d\theta \,\, (a + b \theta) = b, \,\, a,b \in \mathbb{C}
\end{equation}

\noindent Note the very interesting property here that, by this definition, the integral and the derivative are the exact same thing

\begin{equation}
\int d\theta \, (a + b \theta) = \partial_\theta (a + b \theta).
\end{equation}

\noindent This also holds for the multivariable case, where the highest order term is picked off from the Taylor series

\begin{equation}
\int d\theta_n \dots d\theta_1 \, (f_0 + \sum_{j_1 < \dots < j_p} f_p (j_1, \dots, j_p) \theta_{j_1} \dots \theta_{j_p}) = f_n (1, \dots, n).
\end{equation}

\noindent This is a weird definition, but it works, behaves correctly under change of variables, and does what we need it to do. \\

\textbf{Example:}  $n=2$ independent Grassman numbers

\begin{align}
\int d\theta^* d\theta \, e^{-\lambda \theta^* \theta} &= \int d\theta^* d\theta \, (1-\lambda \theta^* \theta) \\
&= \partial_{\theta^*} \partial_\theta \, (1 - \lambda \theta^* \theta) \\
\int d\theta^* d\theta \, e^{-\lambda \theta^* \theta} &= \lambda
\end{align}

\noindent Note that the order of integration ($\theta$ first, $\theta^*$ second) is by convention.

\subsubsection*{Multivariable Gaussian Integrals}

\noindent Consider the multivariable Gaussian integral

\begin{equation}
I = \int d\theta_1^* d\theta_1 \dots d\theta_n^* d\theta_n \,\, e^{-\sum_{j,k} \theta^*_j B_{jk} \theta_k}
\end{equation}

\noindent Where $B^\dagger = B$ and we diagonalize via $U^\dagger B U = D$. \\

\noindent Make a change of variables $\theta'_j = U_{jk} \theta_k$, where $U$ is a unitary matrix, such that the product of these new Grassman variables is (\textbf{Exercise})

\begin{align}
\theta'_1 \theta'_2 \dots \theta'_n &= \sum_{k_1, \dots, k_n} U_{1 k_1} U_{2 k_2} \dots U_{n k_n} \theta_{k_1} \theta_{k_2} \dots \theta_{k_n} \\
&= \sum_{\pi \in S_n} U_{1 \pi(1)} \dots U_{n \pi(n)} \text{sgn}(\pi) \theta_1 \theta_2 \dots \theta_n \\
\theta'_1 \theta'_2 \dots \theta'_n &= \text{det} (U) \theta_1 \theta_2 \dots \theta_n
\end{align}

\noindent So the Gaussian integral becomes

\begin{align}
I &= \int d{\theta'}_1^* d\theta'_1 \dots d{\theta'}_n^* d\theta'_n \,\, e^{-\sum_{j,k} {\theta'}^*_j (U B U^\dagger )_{jk} \theta'_k} \text{det}(U) \text{det}(U^\dagger) \\
&= \int d{\theta'}_1^* d\theta'_1 \dots d{\theta'}_n^* d\theta'_n \,\, e^{-\lambda_1 {\theta'}_1^* \theta'_1} \dots e^{-\lambda_n {\theta'}_n^* \theta'_n} \cdot (1) \cdot (1) \\
&= \lambda_1 \lambda_2 \dots \lambda_n \\
I &= \text{det} (B)
\end{align}

\noindent Recall that for the normal Gaussian integral case, we got $I = (\text{det}(B))^{-1}$. \\

\noindent (\textbf{Exercise}) The generating functional for the Grassman calculus is, where $J$ is a vector of Grassman numbers,

\begin{align}
Z [J] &= \int d\theta_1^* d\theta_1 \dots d\theta_n^* d\theta_n \,\, e^{-\theta^\dagger B \theta + J^\dagger \theta + \theta^\dagger J} \\
&= e^{J^\dagger B^{-1} J}.
\end{align}

\noindent The moments are (\textbf{Exercise})

\begin{equation}
\int d\theta_1^* d\theta_1 \dots d\theta_n^* d\theta_n \,\, \theta_j \theta_k^* e^{-\theta^\dagger B \theta} = \text{det} (B) \cdot [B^{-1}]_{jk}
\end{equation}

\noindent \textbf{Side}: The mixing of regular and Grassman numbers provides the basis for the supersymmetric method. Consider the Gaussian integral

\begin{align}
\int d \Phi e^{-\Phi^\dagger M \Phi} &= \int dx_1 \dots dx_n dx_1^* \dots dx_n^* \int d\theta_1^* d \theta_1 \dots d \theta_n^* d \theta_n \, e^{-\Phi^\dagger M \Phi} \\
&= (\text{det} A)(\text{det} A)^{-1} \\
\int d \Phi e^{-\Phi^\dagger M \Phi} &= 1
\end{align}

\noindent Where $\Phi = (x_1, \dots, x_n, \theta_1, \dots, \theta_n)$ and $M$ is a $2n \times 2n$ matrix

\begin{equation}
M= \left( \begin{matrix} A & 0 \\ 0 & A \end{matrix}  \right)
\end{equation}